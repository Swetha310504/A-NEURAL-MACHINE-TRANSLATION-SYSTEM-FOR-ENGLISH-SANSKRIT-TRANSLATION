{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10948220,"sourceType":"datasetVersion","datasetId":6809917},{"sourceId":10951606,"sourceType":"datasetVersion","datasetId":6812400}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/sweumavarsh/en-sa?scriptVersionId=232440762\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"pip install evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T12:42:30.125025Z","iopub.execute_input":"2025-03-11T12:42:30.12524Z","iopub.status.idle":"2025-03-11T12:42:36.697821Z","shell.execute_reply.started":"2025-03-11T12:42:30.12522Z","shell.execute_reply":"2025-03-11T12:42:36.696759Z"}},"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.3.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.29.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.17.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.11.12)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2025.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.3\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T12:42:36.698923Z","iopub.execute_input":"2025-03-11T12:42:36.699234Z","iopub.status.idle":"2025-03-11T12:42:36.918224Z","shell.execute_reply.started":"2025-03-11T12:42:36.699209Z","shell.execute_reply":"2025-03-11T12:42:36.91733Z"}},"outputs":[{"name":"stdout","text":"Tue Mar 11 12:42:36 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   35C    P8             11W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   38C    P8             12W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nprint(torch.__version__)  # Check PyTorch version\nprint(torch.version.cuda)  # Check CUDA version\nprint(torch.backends.cudnn.version())  # Check cuDNN version\nprint(torch.cuda.is_available())  # Check if GPU is available\nprint(torch.cuda.device_count())  # Number of GPUs available\nprint(torch.cuda.get_device_name(0))  # GPU name (if available)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T12:42:36.919629Z","iopub.execute_input":"2025-03-11T12:42:36.919939Z","iopub.status.idle":"2025-03-11T12:42:42.955176Z","shell.execute_reply.started":"2025-03-11T12:42:36.919917Z","shell.execute_reply":"2025-03-11T12:42:42.954197Z"}},"outputs":[{"name":"stdout","text":"2.5.1+cu121\n12.1\n90100\nTrue\n2\nTesla T4\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom transformers import M2M100ForConditionalGeneration, M2M100Tokenizer, Trainer, TrainingArguments\nfrom datasets import Dataset\nimport evaluate","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T12:42:42.956701Z","iopub.execute_input":"2025-03-11T12:42:42.957093Z","iopub.status.idle":"2025-03-11T12:43:12.459454Z","shell.execute_reply.started":"2025-03-11T12:42:42.95707Z","shell.execute_reply":"2025-03-11T12:43:12.458708Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Load dataset\nfile_path = \"/kaggle/input/trilingual-dataset/trilingual_dataset.xlsx\"\nxls = pd.ExcelFile(file_path)\ntrain_df = pd.read_excel(xls, sheet_name=\"Train\")\ntest_df = pd.read_excel(xls, sheet_name=\"Test\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T12:43:12.460281Z","iopub.execute_input":"2025-03-11T12:43:12.461073Z","iopub.status.idle":"2025-03-11T12:43:14.216087Z","shell.execute_reply.started":"2025-03-11T12:43:12.46086Z","shell.execute_reply":"2025-03-11T12:43:14.215389Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Preprocessing: Rename columns for consistency\ntrain_df = train_df.rename(columns={\"English\": \"en\", \"Hindi\": \"hi\", \"Sanskrit\": \"sa\"})\ntest_df = test_df.rename(columns={\"English\": \"en\", \"Hindi\": \"hi\", \"Sanskrit\": \"sa\"})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T12:43:14.216819Z","iopub.execute_input":"2025-03-11T12:43:14.217304Z","iopub.status.idle":"2025-03-11T12:43:14.227181Z","shell.execute_reply.started":"2025-03-11T12:43:14.217282Z","shell.execute_reply":"2025-03-11T12:43:14.226353Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Convert to Hugging Face Dataset\ntrain_dataset = Dataset.from_pandas(train_df)\ntest_dataset = Dataset.from_pandas(test_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T12:43:14.228262Z","iopub.execute_input":"2025-03-11T12:43:14.228559Z","iopub.status.idle":"2025-03-11T12:43:14.337735Z","shell.execute_reply.started":"2025-03-11T12:43:14.228529Z","shell.execute_reply":"2025-03-11T12:43:14.337125Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Load Model & Tokenizer\nmodel_name = \"facebook/m2m100_418M\"\ntokenizer = M2M100Tokenizer.from_pretrained(model_name)\nmodel = M2M100ForConditionalGeneration.from_pretrained(model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T12:43:14.340473Z","iopub.execute_input":"2025-03-11T12:43:14.340689Z","iopub.status.idle":"2025-03-11T12:43:28.794059Z","shell.execute_reply.started":"2025-03-11T12:43:14.340671Z","shell.execute_reply":"2025-03-11T12:43:28.792816Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/298 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a27e761e821c427c920bc1082edcbb54"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/3.71M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb336ac3845e43e385dfc95cd128b243"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94f6865214a54499a7d4a5b53ec0b467"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/1.14k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"796bc19c2ace4167bbba9301a4bcb863"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/908 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f33f5e8286f249c9b21ffc9b24b77464"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4485a55e4eba4651a70b6f24074b4923"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/233 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04d6a3e4daed4603882eda5d8ed34ee7"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# Tokenization function\ndef tokenize_function(examples):\n    inputs = tokenizer(examples[\"en\"], padding=\"max_length\", truncation=True, max_length=128)\n    targets = tokenizer(examples[\"sa\"], padding=\"max_length\", truncation=True, max_length=128)\n    inputs[\"labels\"] = targets[\"input_ids\"]\n    return inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T12:43:28.796666Z","iopub.execute_input":"2025-03-11T12:43:28.79697Z","iopub.status.idle":"2025-03-11T12:43:28.801805Z","shell.execute_reply.started":"2025-03-11T12:43:28.796941Z","shell.execute_reply":"2025-03-11T12:43:28.801022Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Tokenize datasets\ntrain_dataset = train_dataset.map(tokenize_function, batched=True)\ntest_dataset = test_dataset.map(tokenize_function, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T12:43:28.80268Z","iopub.execute_input":"2025-03-11T12:43:28.802976Z","iopub.status.idle":"2025-03-11T12:43:38.531399Z","shell.execute_reply.started":"2025-03-11T12:43:28.802945Z","shell.execute_reply":"2025-03-11T12:43:38.529981Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c95c89cdf1b4f60b0ce665907cb221d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2032 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8bcc3e94a5794b85a02ad4bb5c2543c1"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"./m2m100_finetuned_sa\",\n    report_to=\"none\",\n    eval_strategy=\"epoch\",  \n    learning_rate=2e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    save_total_limit=2,\n    push_to_hub=False,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T12:43:38.532986Z","iopub.execute_input":"2025-03-11T12:43:38.533409Z","iopub.status.idle":"2025-03-11T12:43:38.575434Z","shell.execute_reply.started":"2025-03-11T12:43:38.533384Z","shell.execute_reply":"2025-03-11T12:43:38.57414Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Trainer setup\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T12:43:38.576605Z","iopub.execute_input":"2025-03-11T12:43:38.576983Z","iopub.status.idle":"2025-03-11T12:43:42.092002Z","shell.execute_reply.started":"2025-03-11T12:43:38.576952Z","shell.execute_reply":"2025-03-11T12:43:42.090164Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"trainer.train(resume_from_checkpoint=False)  # Restart training instead of resuming","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T12:43:42.093314Z","iopub.execute_input":"2025-03-11T12:43:42.093837Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='645' max='7500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 645/7500 09:02 < 1:36:23, 1.19 it/s, Epoch 0.26/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"pip install rouge_score","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save_pretrained(\"/kaggle/working/m2m100_finetuned_sa\")\ntokenizer.save_pretrained(\"/kaggle/working/m2m100_finetuned_sa\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM\n\nmodel_path = \"/kaggle/working/m2m100_finetuned_sa\"\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.gradient_checkpointing_enable()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluation\nimport evaluate\nbleu = evaluate.load(\"bleu\")\nrouge = evaluate.load(\"rouge\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_metrics(eval_pred):\n    preds, labels = eval_pred\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    \n    bleu_score = bleu.compute(predictions=decoded_preds, references=decoded_labels)\n    rouge_score = rouge.compute(predictions=decoded_preds, references=decoded_labels)\n\n    # Print Scores\n    print(\"\\nEvaluation Metrics\")\n    print(f\"BLEU Score: {bleu_score['bleu']:.4f}\")\n    print(f\"ROUGE Scores: {rouge_score}\")\n    \n    return {\"bleu\": bleu_score, \"rouge\": rouge_score}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate predictions and evaluate\nresults = trainer.evaluate()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"./results\",\n    per_device_eval_batch_size=1,  # Reduce batch size if needed\n    report_to=\"none\",  # Disable Weights & Biases logging\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Extract predictions and labels from the dataset\neval_preds = trainer.predict(trainer.eval_dataset)\n\n# Compute and print scores\ncompute_metrics(eval_preds)\n\n# Print Scores\nprint(\"\\nEvaluation Metrics\")\nprint(f\"BLEU Score: {bleu_score['bleu']:.4f}\")\nprint(f\"ROUGE Scores: {rouge_score}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save Model\nmodel.save_pretrained('/kaggle/working/m2m100-en-hi-sa')\ntokenizer.save_pretrained('/kaggle/working/m2m100-en-hi-sa')\n\n!zip -r /kaggle/working/m2m100-en-hi-sa.zip /kaggle/working/m2m100-en-hi-sa\nprint('Model saved and zipped!')\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n!cp /kaggle/working/m2m100-en-hi-sa.zip /content/drive/MyDrive/\n\nprint(\"Fine-tuning complete! Model saved in drive.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nprint(os.listdir(\"./m2m100_finetuned_sa\")) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModel, AutoTokenizer\n\nmodel_path = \"./m2m100_finetuned_sa\"  \nmodel = AutoModel.from_pretrained(model_path)\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\nprint(\"Model and tokenizer loaded successfully!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(\"./m2m100_finetuned_sa/checkpoint-3750\")\nprint(\"Resumed model from checkpoint-3750\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_path)\ndummy_input = \"Hello, I am Swetha\"\ninputs = tokenizer(dummy_input, return_tensors=\"pt\")\n\noutput_tokens = model.generate(**inputs)\noutput_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n\nprint(output_text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}